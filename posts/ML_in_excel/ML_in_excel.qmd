---
title: "Machine Learning in Excel?"
author: "Leo Kiernan"
date: "2023-04-03"
image: AI_ifthen.jpg
draft: true
toc: true
format: 
  html:
    code-fold: true
categories: [code, analysis, excel, ML, AI, LM, randomForests, classification, regression]
---

# Overview

Can you access Machine Learning in Excel? Read on, this post is dedicated to finding out! On route I hope to demystify some of the most popular and powerful Machine Learning methods of recent years and end up with something that anyone with access to a spreadsheet can use for regression or classification tasks (just so long as they're not too big).

In truth, this post was inspired by the following meme:

![](/posts/ML_in_excel/AI_ifthen.jpg)

This image makes me laugh for many reasons.

-   On the one hand, life isn't that simple! Artificial Intelligence (AI) is a huge field that spans from experts systems with fixed and well-defined inference logic through to the almost impenetrable (but extremely useful\|) transforms crafted from data by Machine Learning (ML) paradigms.

-   On the other hand, the image summarises (maybe overly so) a considerable branch of AI and ML. You will find IF-THEN rules in some of the earliest and most recent forms of AI. They underpinned the Expert Systems that emerged in the 1970 and still underpin some of the most successful recent forms of ML.

It's the simplicity and clarity (practicality) that is lovingly mocked in the meme that has allowed the humble "IF THEN" to have survived for 50 years in a technical field that is so fluid.

OK, so history tells us that IF-THEN *is* a credible tool for AI and ML. The Microsoft documentation says that Excel has an IF (this, then-that, otherwise-the-other). So **the door is open**! Note: Excel also has many other logic manipulation functions that I'm not going to touch here but are worth noting, like SWITCH, CHOOSE, and the invaluable and overly used LOOKUP family and INDEX+MATCH). It also has some very powerful ways to search for solutions. If you're not aware, I encourage you to search "goal seek" and "solver" add-ins (bundled with Excel). These are underpinned by a powerful generalised gradient descent algorithms and can answer all sorts of business questions right out of the box through a form of "learning" (fitting / calibration / solving / optimising).

## Lets introduce some ML concepts:

Before getting all technical, I'd like to introduce some of the terms I'll rely on in this post. If you're already verse in decision trees and random forests feel free to skip. If you're genuinely interested in how they work please dig deeper than my description here, I'm just trying to get some big-picture concepts in place as context for the rest of this post.

-   **Decision Trees**: Decision Trees are just a nested sequence of IF-THEN logic. They're quite intuitive. The reason why they're called decision trees is because the help you make decisions and look like trees (The logic shown in the meme show branching conditions that makes it look like a 'tree'). For example, a really small decision tree for 'going out safely' might be:

    -   <div>

        > -   IF \[it's raining\] THEN {bring your umberella}
        >
        > -   ELSE...
        >
        >     -   IF \[it's sunny\] THEN {wear sunscreen}
        >
        >     -   ELSE {you're good to go!}

        </div>

    Note: In the above example, each pair of IF-ELSE generate '**branches**' as the decision can go two ways depending on the result. Moving along the sequence of questions moves along a spcific combination of branches until there are no more questions left to answer. At this point we have tested all the consitions and we can take some action. The bits I've put in {curly brackets} are the actions, because they are at the end of the branches they are called '**leaf**' nodes. The leaf {wear sunscreen} is the action to take IF \[it's not raining\] AND \[it's sunny\]

    You could imagine improving this tree in many ways... You could add all sorts of logic to make the task of "going out safely" more complete (e.g. IF \[it's raining\] THEN IF{it's going to stop soon} THEN \[wait a bit\] ELSE {bring your umbrella} etc.)

-   **Machine learning and decision trees**: It's easy for people to define rules for simple trees but not so easy for complex situations, especially those that are not based on something we already understand. ML decision trees come in to play when we have examples of desired outcomes coupled to a bunch of evidence upon which an outcome can be based. The magic that has been brought by the ML community is *how* to build decisions trees under these circumstanges. There's loads of material on this on-line. I'd point to a great 10 minute video of a guy explaining all of this [here](https://www.youtube.com/watch?v=LDRbO9a6XPU&t=592s).

-   **Random Forests**: At the highest level, random forests are just collections of Decision Trees. Each tree is built using the same learning rules as the next one. If you're wondering why it's useful to have many trees, it's because many trees bring with them different perspectives on the data. Often each tree is built using a sub-set of the data, either by limiting the aspects of the evidence (dimensions) or by limiting the examples (rows). Any one tree may be limited in one way or another, but by combining many trees the weaknesses of any single tree begin to diminish. It's analogous to asking for a second opinion, or in the extreme, "the wisdom of the masses". There's a good article [here](https://towardsdatascience.com/random-forests-walkthrough-why-are-they-better-than-decision-trees-22e02a28c6bd) about just that. Obviously, if you use random forests you may get many different answers for the same question. The ML community have also resolved this, so perhaps if you're looking for a number (i.e. you're regressing) you might chose the average (mean) of all the possible options. Or if you're looking for a label (las per the 'name the fruit' example in that youtube video) you may chose to go with the most popular (modal) result.

In summary... decision trees and random-forests are strewn with IF-THEN logic.

## Sanity check

Pausing for a moment, before diving any further into a post that considered Machine learning and Excel...Another internet meme springs into the mind at this point:

![](but_why_should_you.JPG)

I'm not sure I'll ever be able to answer the question "but... why?" other than by making three statements (with the usual disclaimers around 'The postings on this site are my own and do not necessarily reflect the views of \[Employer\].' etc.)

-   That scooby-doo meme started an itch I had to scratch. Surely decision trees can be delivered in excel

-   (personal development) The journey of trying to drop ML like random forests into Excel helps to demystify ML and brings challenges that were sufficiently non-trivial to feel worthwhile.

-   Finally, and probably most significantly: ML is useful and Excel is ubiquitous, they rarely overlap, is there value in bridging the gap between the two? Builds bridges between the two communities may increase tolerance or reduce friction between the two. The reduction of corporate silos can lead to all sorts of innovations... who knows, maybe ML can be delivered in Excel. Maybe Excel users will become open to alternative ways to analyse data. There's a great video that's well worth a watch exploring such things from JD Long [here](https://www.youtube.com/watch?v=CjNOSrfQiAY).

Most of the rest of this post focusses on build ML models (for both classification and regression) and then how to get these models into excel. however I will need to build these models on *something*. so there's also a lead-in section exploring the data on which I'll be building the models. I'd recommend reading this section even though it's not directly about ML or Excel. It will give some insight into what it takes to make good classification and regression models and contectualise the content of the other sections.

# Preamble (where I explain what I'm going to be working on)

For this post I've chosen a dataset called "Iris". It's one of those "hello world" that is small, but it's well understood and can be used to expo ML for both regression *and* classification. The methods I'm exploring work for larger datasets (more fields and more examples). I'll make some observations on the limits of applying ML in Excel towards the end of this blog.

As ever, before I get started I'm going to load some libraries that I'll be using throughout this post[^1].

[^1]: In general you may not know up-front what packages you need. packages come into play ask you move through the various phases of load-explore--model-summarise-export. The list of packages has got longer as I've attempted more and more steps in this post. I could've loaded each library at the point I need them in the code (or references them using {package}::{function} as per the stringi example in one of my functions) but I've chosen to load them all up-front just as a convention.

```{r load libraries, warning=FALSE, message=FALSE}
library(magrittr)
library(tidyverse)
library(tidypredict)
library(tidymodels)
library(rsample)
library(parsnip)
library(recipes)
library(ranger)
```

## Introducing the dataset I'll be using throughout

I'm using a dataset called 'iris' in this post because it's simple, available and can be used for either classification or regression. It contains 150 examples of measurements taken from different types of iris. Each row is an example of a single iris from one of three species. The dataset holds stats on each iris like the length and width of sepals and petal.

```{r quick look at iris }
head(iris, 5)
```

We can see that there are five things measured on each iris (eah row of the dataset). There are four numeric measurements (**length** and **width** of **sepals** and **petals**) and one categorical (factor) describing the **species** of the iris.

I am going to do two kinds of modelling:

-   Classification: I am going to learn how to tell the **species** of an iris based on the size of the petal and sepals[^2]

-   Regression: I'm going to learn how to estimate the **length of the petals** based on the species and other attributes.

[^2]: The dataset is relatively simple to feed into a classifier because the classes (species) are 'balanced' (which just means there's the same number of examples in each class). The ML community have developed plenty of ways that to handle unbalanced classes but anyone undertaking ML classification must consider this when constructing any ML solution.

## Making the data a little more interesting

In some ways, the iris dataset is a little too clean to highlight anything interesting about the machine learning process... So I'm going to add two extra columns (measurements) to each iris to see if/how the ML algorithms decide to use these in the classification and regression tasks:

-   **Noise**: Noise will be entirely random, unrelated to the characteristics of each iris in any way. I'll use this as a test of the algorithms. Because the number is unrelated to the iris it should have no predictive power to help me classify species or estimate petal length, so it shouldnt be included in the decision making processes.

-   **Noisy.Sepal.Length**: This is a *little* related to an attribute of each iris. It's like the measurement of the length of the sepal but taken in a really sloppy way where it's the true length but with a random number added or subtracted to make the measurement almost useless. This can never be as useful as the clean version of Sepal.Length, but compared to **Noise**, this measure contains *some* value.

```{r augment the iris dataset with some useless and nearly uesless extra columns}
# iris_n_noise ----
# create (augment) the dataset we'll be using through this piece
iris_n_noise <- iris %>%
  as_tibble() %>%
  add_column (Noise = runif (nrow (.))) %>%
  add_column (other_noise = runif (nrow (.))) %>%
  mutate(Noisy.Sepal.Length = Sepal.Length + 10.0*other_noise) %>%
  select(-other_noise)

```

Let's have a quick look at the resultant dataset on which I'll be using. The plots below may be overwhelming to start with but they are really useful. I've deliberately asked for data for different species to be plotted in differnent colours to help interpret the data as one of the things we're going to want to do is to classify species. I'll interpret them once I've plotted them.

```{r,  generate pairs plots of the augmented iris dataset, warning=FALSE, message=FALSE}
GGally::ggpairs(iris_n_noise, mapping = aes(color = Species))
```

There's a lot in this plot but in essence it compares each column against all other columns and presents the results in a couple of different ways. There's deinsity plots on the diagonal, scatterplots below the diagonal and other useful stats above the diagonal. I'll focus on the sub-plots that are most illustrative for the classification / regression problems:

-   Classification of species:

# Machine Learning and Excel

## Classification

asrrf

```{r}

```

## Regression

dfg

### Regression using LM (Linear models)

First off, I'd just like to state that linear models and generalised linear models *are* a form of machine learning. They generate information from data. I'm deliberately(ish) adding this section on regression in LMs because they are more familiar to many than other ML techniques and act as a valuable reference point against which other ML techniques can be compared. And yes, I appreciate that Excel already has capability to do linear regression, but I'm trying to highlight bridges here, one step beyond preparing the data to be fitted in excel is doing the fit elsewhere and feeding the fitted model into Excel for on-use.

```{r}

```

### Regression using ML (machine learning)

sdfg

```{r}

```

## Getting the models into Excel

sdfdfgsdfg

# Summary

dfg
